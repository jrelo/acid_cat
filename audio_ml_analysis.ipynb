{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Sample Analysis and ML Experiments\n",
    "\n",
    "This notebook demonstrates how to use the enhanced `acid_cat` tool for machine learning experiments on audio sample libraries.\n",
    "\n",
    "## Features:\n",
    "- Load and explore extracted audio features\n",
    "- Visualize feature distributions and relationships\n",
    "- Perform similarity analysis\n",
    "- Cluster samples by audio characteristics\n",
    "- Build recommendation systems\n",
    "\n",
    "## Prerequisites:\n",
    "Run `acid_cat.py` with `--ml-ready` flag to generate feature-rich CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio features dataset\n",
    "# Replace 'your_metadata.csv' with your actual CSV file path\n",
    "# df = pd.read_csv('your_metadata.csv')\n",
    "\n",
    "# For demo purposes, let's load the sample data\n",
    "try:\n",
    "    df = pd.read_csv('samples_metadata.csv')\n",
    "    print(f\"Loaded {len(df)} audio samples\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\nexcept FileNotFoundError:\n",
    "    print(\"CSV file not found. Please run: python acid_cat.py data/samples --ml-ready\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Basic info about the dataset\n",
    "    print(\"Dataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nBasic statistics:\")\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Identify numeric columns for analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"Found {len(numeric_cols)} numeric features:\")\n",
    "    print(numeric_cols)\n",
    "    \n",
    "    # Remove problematic columns that might contain lists\n",
    "    feature_cols = [col for col in numeric_cols if not any(x in col.lower() for x in ['tempo_librosa'])]\n",
    "    print(f\"\\nUsing {len(feature_cols)} features for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and len(df) > 1:\n",
    "    # Distribution of key audio features\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Distribution of Key Audio Features', fontsize=16)\n",
    "    \n",
    "    features_to_plot = ['bpm', 'duration_sec', 'spectral_centroid_mean', \n",
    "                       'mfcc_1_mean', 'rms_mean', 'zcr_mean']\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        if feature in df.columns:\n",
    "            ax = axes[i//3, i%3]\n",
    "            df[feature].hist(bins=20, ax=ax, alpha=0.7)\n",
    "            ax.set_title(f'{feature}')\n",
    "            ax.set_xlabel(feature)\n",
    "            ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"Need more than one sample for meaningful visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and len(feature_cols) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[feature_cols].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0,\n",
    "                square=True, cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Audio Feature Correlation Matrix')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.8:\n",
    "                high_corr_pairs.append((correlation_matrix.columns[i], \n",
    "                                      correlation_matrix.columns[j], \n",
    "                                      corr_val))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"\\nHighly correlated feature pairs (|r| > 0.8):\")\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nNo highly correlated feature pairs found (|r| > 0.8)\")\nelse:\n",
    "    print(\"Need multiple features and samples for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and len(df) > 1 and len(feature_cols) > 2:\n",
    "    # Prepare data for dimensionality reduction\n",
    "    X = df[feature_cols].fillna(0)  # Fill NaN values\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # t-SNE (only if we have enough samples)\n",
    "    if len(df) >= 5:\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(df)-1))\n",
    "        X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2 if len(df) >= 5 else 1, figsize=(15, 6))\n",
    "    if len(df) < 5:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # PCA plot\n",
    "    axes[0].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7, s=50)\n",
    "    axes[0].set_title(f'PCA Visualization\\n(Explained variance: {pca.explained_variance_ratio_.sum():.2%})')\n",
    "    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    \n",
    "    # t-SNE plot\n",
    "    if len(df) >= 5:\n",
    "        axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7, s=50)\n",
    "        axes[1].set_title('t-SNE Visualization')\n",
    "        axes[1].set_xlabel('t-SNE 1')\n",
    "        axes[1].set_ylabel('t-SNE 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\nelse:\n",
    "    print(\"Need more samples and features for dimensionality reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Audio Sample Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and len(df) > 2 and len(feature_cols) > 2:\n",
    "    # Prepare data\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # K-Means clustering\n",
    "    n_clusters = min(3, len(df))  # Adjust based on sample size\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add cluster labels to dataframe\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered['cluster'] = cluster_labels\n",
    "    \n",
    "    # Visualize clusters in PCA space\n",
    "    if 'X_pca' in locals():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, \n",
    "                            cmap='viridis', alpha=0.7, s=50)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(f'K-Means Clustering (k={n_clusters}) in PCA Space')\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "        \n",
    "        # Add sample names as annotations\n",
    "        for i, filename in enumerate(df['filename']):\n",
    "            plt.annotate(filename.split('/')[-1].split('\\\\')[-1][:10], \n",
    "                        (X_pca[i, 0], X_pca[i, 1]), \n",
    "                        xytext=(5, 5), textcoords='offset points', \n",
    "                        fontsize=8, alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Cluster characteristics\n",
    "    print(\"\\nCluster Characteristics:\")\n",
    "    key_features = ['bpm', 'duration_sec', 'spectral_centroid_mean', 'mfcc_1_mean']\n",
    "    cluster_summary = df_clustered.groupby('cluster')[key_features].mean()\n",
    "    print(cluster_summary)\n",
    "    \n",
    "    print(\"\\nSamples per cluster:\")\n",
    "    print(df_clustered['cluster'].value_counts().sort_index())\nelse:\n",
    "    print(\"Need more samples for meaningful clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Audio Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_samples(df, target_idx, feature_cols, n_similar=5):\n",
    "    \"\"\"\n",
    "    Find samples similar to the target sample using cosine similarity.\n",
    "    \"\"\"\n",
    "    if len(df) < 2:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Prepare features\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    target_features = X_scaled[target_idx].reshape(1, -1)\n",
    "    similarities = cosine_similarity(target_features, X_scaled)[0]\n",
    "    \n",
    "    # Get most similar samples (excluding the target itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:n_similar+1]\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = df.iloc[similar_indices].copy()\n",
    "    results['similarity_score'] = similarities[similar_indices]\n",
    "    \n",
    "    return results[['filename', 'bpm', 'duration_sec', 'similarity_score']]\n",
    "\n",
    "if df is not None and len(df) > 1:\n",
    "    # Example: Find samples similar to the first sample\n",
    "    target_idx = 0\n",
    "    target_sample = df.iloc[target_idx]['filename']\n",
    "    \n",
    "    print(f\"Finding samples similar to: {target_sample}\")\n",
    "    print(f\"Target BPM: {df.iloc[target_idx]['bpm']}\")\n",
    "    print(f\"Target Duration: {df.iloc[target_idx]['duration_sec']:.2f}s\")\n",
    "    \n",
    "    similar_samples = find_similar_samples(df, target_idx, feature_cols, n_similar=3)\n",
    "    \n",
    "    if not similar_samples.empty:\n",
    "        print(\"\\nMost similar samples:\")\n",
    "        print(similar_samples)\n",
    "    else:\n",
    "        print(\"\\nNo similar samples found (need more data)\")\nelse:\n",
    "    print(\"Need multiple samples for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and len(df) > 1 and len(feature_cols) > 2:\n",
    "    # Use PCA to understand feature importance\n",
    "    X = df[feature_cols].fillna(0)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(X_scaled)\n",
    "    \n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "             pca_full.explained_variance_ratio_, 'bo-')\n",
    "    plt.title('PCA Explained Variance by Component')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    plt.plot(range(1, len(cumsum) + 1), cumsum, 'ro-')\n",
    "    plt.title('Cumulative Explained Variance')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.axhline(y=0.95, color='k', linestyle='--', alpha=0.7, label='95%')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find number of components for 95% variance\n",
    "    n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "    print(f\"\\nNumber of components needed for 95% variance: {n_components_95}\")\n",
    "    print(f\"Total features: {len(feature_cols)}\")\n",
    "    print(f\"Dimensionality reduction: {len(feature_cols)} -> {n_components_95} ({n_components_95/len(feature_cols)*100:.1f}%)\")\nelse:\n",
    "    print(\"Need more samples and features for importance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sample Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSampleRecommender:\n",
    "    def __init__(self, df, feature_cols):\n",
    "        self.df = df.copy()\n",
    "        self.feature_cols = feature_cols\n",
    "        self.scaler = StandardScaler()\n",
    "        self.features_scaled = None\n",
    "        self.nn_model = None\n",
    "        self._prepare_data()\n",
    "    \n",
    "    def _prepare_data(self):\n",
    "        # Prepare and scale features\n",
    "        X = self.df[self.feature_cols].fillna(0)\n",
    "        self.features_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Initialize nearest neighbors model\n",
    "        self.nn_model = NearestNeighbors(n_neighbors=min(5, len(self.df)), \n",
    "                                        metric='cosine')\n",
    "        self.nn_model.fit(self.features_scaled)\n",
    "    \n",
    "    def recommend_by_sample(self, sample_idx, n_recommendations=3):\n",
    "        \"\"\"Recommend samples similar to a given sample.\"\"\"\n",
    "        if len(self.df) < 2:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        target_features = self.features_scaled[sample_idx].reshape(1, -1)\n",
    "        distances, indices = self.nn_model.kneighbors(target_features, \n",
    "                                                     n_neighbors=n_recommendations+1)\n",
    "        \n",
    "        # Exclude the target sample itself\n",
    "        similar_indices = indices[0][1:]\n",
    "        similarities = 1 - distances[0][1:]  # Convert distance to similarity\n",
    "        \n",
    "        results = self.df.iloc[similar_indices].copy()\n",
    "        results['similarity_score'] = similarities\n",
    "        \n",
    "        return results[['filename', 'bpm', 'duration_sec', 'similarity_score']]\n",
    "    \n",
    "    def recommend_by_criteria(self, target_bpm=None, target_duration=None, \n",
    "                            target_key=None, n_recommendations=3):\n",
    "        \"\"\"Recommend samples based on musical criteria.\"\"\"\n",
    "        candidates = self.df.copy()\n",
    "        \n",
    "        # Filter by criteria\n",
    "        if target_bpm is not None:\n",
    "            bpm_tolerance = 10  # Allow ±10 BPM\n",
    "            candidates = candidates[\n",
    "                (candidates['bpm'] >= target_bpm - bpm_tolerance) & \n",
    "                (candidates['bpm'] <= target_bpm + bpm_tolerance)\n",
    "            ]\n",
    "        \n",
    "        if target_duration is not None:\n",
    "            duration_tolerance = 2.0  # Allow ±2 seconds\n",
    "            candidates = candidates[\n",
    "                (candidates['duration_sec'] >= target_duration - duration_tolerance) & \n",
    "                (candidates['duration_sec'] <= target_duration + duration_tolerance)\n",
    "            ]\n",
    "        \n",
    "        if target_key is not None:\n",
    "            candidates = candidates[\n",
    "                (candidates['smpl_root_key'] == target_key) |\n",
    "                (candidates['acid_root_note'] == target_key)\n",
    "            ]\n",
    "        \n",
    "        # Return top matches\n",
    "        return candidates.head(n_recommendations)[['filename', 'bpm', 'duration_sec', \n",
    "                                                  'smpl_root_key', 'acid_root_note']]\n",
    "\n",
    "# Create recommender system\n",
    "if df is not None and len(df) > 1:\n",
    "    recommender = AudioSampleRecommender(df, feature_cols)\n",
    "    \n",
    "    print(\"=== Audio Sample Recommender ===\")\n",
    "    print(\"\\n1. Similarity-based recommendations:\")\n",
    "    similar_recs = recommender.recommend_by_sample(0, n_recommendations=2)\n",
    "    print(similar_recs)\n",
    "    \n",
    "    print(\"\\n2. Criteria-based recommendations:\")\n",
    "    # Example: Find samples around 95 BPM\n",
    "    criteria_recs = recommender.recommend_by_criteria(target_bpm=95, n_recommendations=3)\n",
    "    print(criteria_recs)\nelse:\n",
    "    print(\"Need multiple samples to create a recommender system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export and Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Save analysis results\n",
    "    analysis_results = {\n",
    "        'dataset_info': {\n",
    "            'total_samples': len(df),\n",
    "            'total_features': len(feature_cols),\n",
    "            'avg_duration': df['duration_sec'].mean() if 'duration_sec' in df.columns else None,\n",
    "            'avg_bpm': df['bpm'].mean() if 'bpm' in df.columns else None\n",
    "        },\n",
    "        'feature_columns': feature_cols\n",
    "    }\n",
    "    \n",
    "    print(\"Analysis Summary:\")\n",
    "    print(f\"Total samples analyzed: {analysis_results['dataset_info']['total_samples']}\")\n",
    "    print(f\"Total features extracted: {analysis_results['dataset_info']['total_features']}\")\n",
    "    \n",
    "    if analysis_results['dataset_info']['avg_duration']:\n",
    "        print(f\"Average duration: {analysis_results['dataset_info']['avg_duration']:.2f} seconds\")\n",
    "    \n",
    "    if analysis_results['dataset_info']['avg_bpm']:\n",
    "        print(f\"Average BPM: {analysis_results['dataset_info']['avg_bpm']:.1f}\")\n",
    "    \n",
    "    # Save enhanced dataset with cluster labels if available\n",
    "    if 'df_clustered' in locals():\n",
    "        df_clustered.to_csv('audio_analysis_with_clusters.csv', index=False)\n",
    "        print(\"\\nSaved enhanced dataset with cluster labels to: audio_analysis_with_clusters.csv\")\n",
    "    \n",
    "    print(\"\\n=== Next Steps ===\")\n",
    "    print(\"1. Run acid_cat on larger sample libraries for richer analysis\")\n",
    "    print(\"2. Experiment with different clustering algorithms\")\n",
    "    print(\"3. Build custom similarity metrics for your specific use case\")\n",
    "    print(\"4. Create playlists or collections based on similarity clusters\")\n",
    "    print(\"5. Train ML models for automatic sample classification\")\nelse:\n",
    "    print(\"No data to analyze. Please run acid_cat with --ml-ready flag first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Analysis Ideas\n",
    "\n",
    "This notebook provides a foundation for audio sample analysis. Here are some advanced ideas to explore:\n",
    "\n",
    "### Machine Learning Models\n",
    "- **Genre Classification**: Train classifiers to predict music genres\n",
    "- **Mood Detection**: Analyze emotional content of samples\n",
    "- **Instrument Recognition**: Identify dominant instruments in samples\n",
    "\n",
    "### Feature Engineering\n",
    "- **Rhythmic Patterns**: Extract beat patterns and rhythmic complexity\n",
    "- **Harmonic Analysis**: Analyze chord progressions and harmonic content\n",
    "- **Dynamic Range**: Measure loudness variations and dynamics\n",
    "\n",
    "### Recommendation Systems\n",
    "- **Collaborative Filtering**: Learn from user preferences\n",
    "- **Content-Based Filtering**: Use audio features for recommendations\n",
    "- **Hybrid Systems**: Combine multiple recommendation approaches\n",
    "\n",
    "### Visualization\n",
    "- **Audio Spectrograms**: Visualize frequency content over time\n",
    "- **3D Feature Spaces**: Explore high-dimensional feature relationships\n",
    "- **Interactive Dashboards**: Build web interfaces for sample exploration\n",
    "\n",
    "### Production Tools\n",
    "- **Sample Matching**: Find samples that work well together\n",
    "- **Key/Tempo Compatibility**: Suggest harmonically compatible samples\n",
    "- **Loop Generation**: Create new loops based on existing patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}